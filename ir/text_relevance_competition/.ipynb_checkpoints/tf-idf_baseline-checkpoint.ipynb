{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF-IDF baseline\n",
    "\n",
    "В данном ноутбуке реализовано решение с помощью TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michelle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "PATH = \"/Users/michelle/data/text-relevance-competition-ir-1-ts-fall-2020/\"\n",
    "\n",
    "mystem = Mystem()\n",
    "stop_list = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir = os.listdir(os.path.join(PATH, \"texts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF для всех документов документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем считать IDF в двух вариантах: по всей базе документов, и отдельно по той выдаче, которая соответствует конкретному запросу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_count = defaultdict(int)\n",
    "lost_files = []\n",
    "for filename in tqdm(listdir, total=len(listdir)):\n",
    "    try:\n",
    "        if filename.endswith(\".pkl\"):\n",
    "            with open(os.path.join(PATH, \"texts\", filename), \"rb\") as f:\n",
    "                doc = pickle.load(f)\n",
    "            for term in set(doc.split()):\n",
    "                terms_count[term] += 1\n",
    "    except:\n",
    "        lost_files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(query, text, terms_count):\n",
    "    lemmed_query = text_handler(query)\n",
    "    terms = lemmed_query.split()\n",
    "    score = 0\n",
    "    text_terms = text.split()\n",
    "    for term in terms:\n",
    "        tf = text_terms.count(term) / len(text_terms)\n",
    "        if tf == 0:\n",
    "            return 0\n",
    "        idf = len(listdir) / terms_count[term]\n",
    "        score += tf * math.log2(idf)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + \"queries.numerate.txt\", \"r\") as f:\n",
    "    queries = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(PATH + \"sample.technosphere.ir1.textrelevance.submission.txt\", \"r\") as f:\n",
    "    sample = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим sample в словарь запрос-документы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091c69a05c264ad786915a8ba6823644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38773.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_docid = defaultdict(list)\n",
    "for item in tqdm(sample[1:], total=len(sample) - 1):\n",
    "    if item:\n",
    "        qid = item.split(\",\")[0]\n",
    "        docid = item.split(\",\")[1]\n",
    "        query_docid[qid].append(docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(PATH, \"inv_index\"), \"wb\") as f:\n",
    "#     pickle.dump(terms_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH, \"inv_index\"), \"rb\") as f:\n",
    "    terms_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [None] + [item.split(\"\\t\")[1] for item in queries if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir = set(listdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходим по запросам и соответствующим документам, считаем для каждой пары TF-IDF, ранжируем и пишем в словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7df66ff1614425b93ed050207b1202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=399.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_query_docid = {}\n",
    "for item in tqdm(query_docid.items(), total=len(query_docid)):\n",
    "    query = queries[int(item[0])]\n",
    "    docids = item[1]\n",
    "    result = []\n",
    "    for docid in docids:\n",
    "        filename = f\"{docid}.pkl_text.pkl\"\n",
    "        if filename in listdir:\n",
    "            with open(os.path.join(PATH, \"texts\", filename), \"rb\") as f:\n",
    "                text = pickle.load(f)\n",
    "            score = get_tf_idf(query, text, terms_count)\n",
    "        else:\n",
    "            score = 0\n",
    "        result.append((score, docid))\n",
    "    result.sort(reverse=True)\n",
    "    new_query_docid[item[0]] = [doc_item[1] for doc_item in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = ['QueryId,DocumentId']\n",
    "for i in range(1, len(queries)):\n",
    "    docids = new_query_docid[str(i)]\n",
    "    for docid in docids:\n",
    "        submit.append(f\"{i},{docid}\")\n",
    "with open(PATH + \"submit_19_01.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(submit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF для подмножества документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем считать IDF слов по подможеству документов для каждого запроса. Далее делаем то же самое, что и в случае выше, только передаем другой словарь с IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7bf1778c9e4af9aaf13a4f458f27ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "terms_count_dicts = []\n",
    "lost_files = []\n",
    "for item in tqdm(query_docid.items(), total=len(queries)):\n",
    "    terms_count = defaultdict(int)\n",
    "    for docid in item[1]:\n",
    "        filename = f\"{docid}.pkl_text.pkl\"\n",
    "        try:\n",
    "            with open(os.path.join(PATH, \"texts\", filename), \"rb\") as f:\n",
    "                doc = pickle.load(f)\n",
    "            for term in set(doc.split()):\n",
    "                terms_count[term] += 1\n",
    "        except:\n",
    "            lost_files.append(docid)\n",
    "    terms_count_dicts.append(terms_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aa07c02c6b49e18c67b01653450971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=399.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_query_docid = {}\n",
    "for item in tqdm(query_docid.items(), total=len(query_docid)):\n",
    "    query = queries[int(item[0])]\n",
    "    docids = item[1]\n",
    "    result = []\n",
    "    for docid in docids:\n",
    "        filename = f\"{docid}.pkl_text.pkl\"\n",
    "        if filename in listdir:\n",
    "            with open(os.path.join(PATH, \"texts\", filename), \"rb\") as f:\n",
    "                text = pickle.load(f)\n",
    "            score = get_tf_idf(query, text, terms_count_dicts[int(item[0]) - 1])\n",
    "        else:\n",
    "            score = 0\n",
    "        result.append((score, docid))\n",
    "    result.sort(reverse=True)\n",
    "    new_query_docid[item[0]] = [doc_item[1] for doc_item in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = ['QueryId,DocumentId']\n",
    "for i in range(1, len(queries)):\n",
    "    docids = new_query_docid[str(i)]\n",
    "    for docid in docids:\n",
    "        submit.append(f\"{i},{docid}\")\n",
    "with open(PATH + \"submit_19_03.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(submit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
