{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from smart_parser import clean_text, lemmatize_text\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "PATH = \"/Users/michelle/data/text-relevance-competition-ir-1-ts-fall-2020/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Mystem()\n",
    "\n",
    "with open(os.path.join(PATH, \"stop_words.pkl\"), \"rb\") as f:\n",
    "    stop_words = set(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем и преобразуем данные для запросов, считаем IDF слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6e9f2fa419465da89f634fc3f417af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(PATH + \"queries.numerate.txt\", \"r\") as f:\n",
    "    queries = f.read().split(\"\\n\")\n",
    "    \n",
    "with open(PATH + \"sample.technosphere.ir1.textrelevance.submission.txt\", \"r\") as f:\n",
    "    sample = f.read().split(\"\\n\")\n",
    "    \n",
    "query_docid = defaultdict(list)\n",
    "for item in sample[1:]:\n",
    "    if item:\n",
    "        qid = item.split(\",\")[0]\n",
    "        docid = item.split(\",\")[1]\n",
    "        query_docid[qid].append(docid)\n",
    "        \n",
    "queries = [None] + [item.split(\"\\t\")[1] for item in queries if item]\n",
    "\n",
    "listdir = os.listdir(os.path.join(PATH, \"parsed_content\"))\n",
    "listdir = set(listdir)\n",
    "\n",
    "terms_count_dicts = []\n",
    "lost_files = []\n",
    "for item in tqdm(query_docid.items(), total=len(queries)):\n",
    "    terms_count = defaultdict(int)\n",
    "    for docid in item[1]:\n",
    "        filename = f\"parsed_{docid}.pkl\"\n",
    "        try:\n",
    "            with open(os.path.join(PATH, \"parsed_content\", filename), \"rb\") as f:\n",
    "                doc = pickle.load(f)[0]\n",
    "            for term in set(doc.split()):\n",
    "                terms_count[term] += 1\n",
    "        except:\n",
    "            lost_files.append(docid)\n",
    "    terms_count_dicts.append(terms_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH, \"term_count_subsets\"), \"wb\") as f:\n",
    "    pickle.dump(terms_count_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_len = 8682.4\n",
    "corpus_len = 38115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая функция считает tf для каждого поля, складывает их с весами, а потом поставляет в BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25f_score(query, docid, terms_count, k1=1.2, k2=1000, b=0.75):\n",
    "    lemmed_query_list = lemmatize_text(clean_text(query), stemmer).split()\n",
    "    score = 0\n",
    "    \n",
    "    filename = f\"parsed_{docid}.pkl\"\n",
    "    if filename in listdir:\n",
    "        with open(os.path.join(PATH, \"parsed_content\", filename), \"rb\") as f:\n",
    "            doc = pickle.load(f)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    text = doc[0].split()\n",
    "    title = doc[1].split()\n",
    "    meta = doc[2].split()\n",
    "    headers = [header.split() for header in doc[3]]\n",
    "    \n",
    "    for term in set(lemmed_query_list):\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        idf = math.log((corpus_len - terms_count[term] + 0.5) / (terms_count[term] + 0.5))\n",
    "        \n",
    "        text_tf = text.count(term)\n",
    "        title_tf = title.count(term)\n",
    "        meta_tf = meta.count(term)\n",
    "        headers_tf = [header.count(term) for header in headers]\n",
    "        \n",
    "        \n",
    "        \n",
    "        tf = 0.1 * text_tf + 10 * title_tf + 10 * meta_tf\n",
    "        for coef, item in enumerate(headers_tf):\n",
    "            tf += 6 / (coef + 1) * item\n",
    "        \n",
    "        qf = lemmed_query_list.count(term)\n",
    "        dl = len(text)\n",
    "        K = k1 * ((1 - b) + b * dl / average_doc_len)\n",
    "        \n",
    "        score += idf * (k1 + 1) * tf / (K + tf) * ((k2 + 1) * qf / (k2 + qf))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c6d8588c224679afc3e75626efb475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=399.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_query_docid = {}\n",
    "for item in tqdm(query_docid.items(), total=len(query_docid)):\n",
    "    query = queries[int(item[0])]\n",
    "    docids = item[1]\n",
    "    result = []\n",
    "    for docid in docids:\n",
    "        score = bm25f_score(query, docid, terms_count_dicts[int(item[0]) - 1])\n",
    "        result.append((score, docid))\n",
    "    result.sort(reverse=True)\n",
    "    new_query_docid[item[0]] = [doc_item[1] for doc_item in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = ['QueryId,DocumentId']\n",
    "for i in range(1, len(queries)):\n",
    "    docids = new_query_docid[str(i)]\n",
    "    for docid in docids:\n",
    "        submit.append(f\"{i},{docid}\")\n",
    "        \n",
    "with open(PATH + \"submit_20_2.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(submit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сначала BM25, потом сложить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по лидерборду, предыдущий пример не дал серьезного прироста качества, поэтому я попробовал сначала пересчитать tf через BM25 для каждого поля, а потом сложить с весами (получилось лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25f_score_base(query, docid, terms_count, k1=1, k2=0.001):\n",
    "    lemmed_query_list = lemmatize_text(clean_text(query), stemmer).split()\n",
    "    score = 0\n",
    "    \n",
    "    text_weight = 1\n",
    "    headers_weight = 1.5\n",
    "    meta_weight = 4\n",
    "    title_weight = 5\n",
    "    \n",
    "    filename = f\"parsed_{docid}.pkl\"\n",
    "    if filename in listdir:\n",
    "        with open(os.path.join(PATH, \"parsed_content\", filename), \"rb\") as f:\n",
    "            doc = pickle.load(f)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    text = doc[0].split()\n",
    "    title = doc[1].split()\n",
    "    meta = doc[2].split()\n",
    "    headers = [header.split() for header in doc[3]]\n",
    "    \n",
    "    for term in set(lemmed_query_list):\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        \n",
    "        idf = -math.log((terms_count[term] + 1) / corpus_len)\n",
    "        \n",
    "        text_tf = text.count(term)\n",
    "        title_tf = title.count(term)\n",
    "        meta_tf = meta.count(term)\n",
    "        headers_tf = sum([header.count(term) for header in headers])\n",
    "        \n",
    "        dl = len(text)\n",
    "        \n",
    "        text_tf = text_tf / (text_tf + k1 + k2 * dl)\n",
    "        title_tf = title_tf / (title_tf + k1 + k2 * dl)\n",
    "        meta_tf = meta_tf / (meta_tf + k1 + k2 * dl)\n",
    "        headers_tf = headers_tf / (headers_tf + k1 + k2 * dl)\n",
    "        \n",
    "        score += idf * (text_tf + title_weight * title_tf + meta_weight * meta_tf + header_weight * headers_tf)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76a4f04083e4f25a3c90e64f167085c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=399.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_query_docid = {}\n",
    "for item in tqdm(query_docid.items(), total=len(query_docid)):\n",
    "    query = queries[int(item[0])]\n",
    "    docids = item[1]\n",
    "    result = []\n",
    "    for docid in docids:\n",
    "        score = bm25f_score_base(query, docid, terms_count_dicts[int(item[0]) - 1])\n",
    "        result.append((score, docid))\n",
    "    result.sort(reverse=True)\n",
    "    new_query_docid[item[0]] = [doc_item[1] for doc_item in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = ['QueryId,DocumentId']\n",
    "for i in range(1, len(queries)):\n",
    "    docids = new_query_docid[str(i)]\n",
    "    for docid in docids:\n",
    "        submit.append(f\"{i},{docid}\")\n",
    "        \n",
    "with open(PATH + \"submit_20_3.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(submit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
